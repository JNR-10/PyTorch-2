{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for dataset and image processing\n",
    "import torch  # PyTorch main package for tensor operations\n",
    "from torch.utils.data import Dataset, DataLoader  # For custom datasets and batching\n",
    "from torchvision.transforms import transforms  # For image transformations\n",
    "import os  # For file and directory operations\n",
    "from PIL import Image  # For image loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class inheriting from PyTorch's Dataset\n",
    "class test_dataset(Dataset):\n",
    "    image_paths = []  # List to store image file paths\n",
    "    \n",
    "    def __init__(self, image_dir):\n",
    "        # Initialize the dataset by scanning the directory for image files\n",
    "        self.image_paths = []\n",
    "        # Loop through all files in the given directory\n",
    "        for file in os.listdir(image_dir):\n",
    "            # Check if the file is a .jpg or .png image\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                self.image_paths.append(file)  # Add image file to the list\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of images in the dataset\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Define a sequence of image transformations to apply\n",
    "        image_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.1),  # Randomly flip image horizontally with 10% probability\n",
    "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),  # Randomly change brightness, contrast, etc.\n",
    "            transforms.CenterCrop((300, 300)),  # Crop the center of the image to 300x300 pixels\n",
    "            transforms.ToTensor() # Convert image to PyTorch tensor and scale between 0 and 1\n",
    "        ])\n",
    "        image_path = self.image_paths[index]  # Get the image file path for the given index\n",
    "        img = Image.open(image_path)  # Open the image using PIL\n",
    "\n",
    "        img_tensor = image_transform(img)  # Apply the transformations to the image\n",
    "        return img_tensor  # Return the transformed image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb411047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0314, 0.0980, 0.3804,  ..., 0.1882, 0.1882, 0.1922],\n",
       "         [0.0196, 0.1176, 0.2745,  ..., 0.2000, 0.2118, 0.2000],\n",
       "         [0.1882, 0.2902, 0.1373,  ..., 0.2118, 0.1686, 0.1569],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.2784, 0.3647, 0.3216],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.3765, 0.3451, 0.1922],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1922, 0.3569, 0.2745]],\n",
       "\n",
       "        [[0.0235, 0.0863, 0.3647,  ..., 0.1255, 0.1176, 0.1255],\n",
       "         [0.0078, 0.1098, 0.2706,  ..., 0.1333, 0.1451, 0.1176],\n",
       "         [0.1882, 0.2902, 0.1490,  ..., 0.1333, 0.0902, 0.0706],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.2706, 0.3451, 0.3059],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.3608, 0.3255, 0.1725],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1686, 0.3176, 0.2392]],\n",
       "\n",
       "        [[0.0000, 0.0078, 0.2392,  ..., 0.5059, 0.5098, 0.5020],\n",
       "         [0.0000, 0.0118, 0.1490,  ..., 0.5294, 0.5294, 0.5098],\n",
       "         [0.0863, 0.1765, 0.0471,  ..., 0.5333, 0.4941, 0.4706],\n",
       "         ...,\n",
       "         [0.0863, 0.0902, 0.0980,  ..., 0.2902, 0.3451, 0.2706],\n",
       "         [0.0863, 0.0941, 0.1059,  ..., 0.3882, 0.3451, 0.1647],\n",
       "         [0.0863, 0.0980, 0.1137,  ..., 0.2314, 0.3843, 0.3137]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the custom dataset using the current directory\n",
    "# This will scan for all .jpg and .png images in the directory\n",
    "# and store their paths in the dataset\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = test_dataset('.')\n",
    "\n",
    "# Retrieve the first image tensor from the dataset\n",
    "# This will apply the defined transformations and return a tensor\n",
    "first_image_tensor = dataset[0]  # get first image tensor\n",
    "first_image_tensor  # Display the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2206e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader to batch and shuffle the dataset\n",
    "# DataLoader helps in loading data in batches and optionally shuffling it for training\n",
    "\n",
    "dl = DataLoader(dataset, batch_size=2, shuffle=True)  # Create DataLoader with batch size 2 and shuffling enabled\n",
    "\n",
    "# Retrieve the next batch of images from the DataLoader\n",
    "output = next(iter(dl))  # Get the first batch (as a tensor of shape [2, 3, 300, 300])\n",
    "print(output.shape) # should be [2, 3, 300, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27fd1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
